---
title: Tool wear detection with a Random Forest classifier
author: Florian Handke
date: '2020-06-26'
slug: tool-wear-detection-with-a-random-forest-classifier
categories:
  - R
  - Data Science
tags:
  - '2020'
  - ranger
  - Random Forest
  - Classifier
subtitle: ''
summary: ''
authors: []
lastmod: '2020-06-26T14:00:41+02:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/plotly-binding/plotly.js"></script>
<script src="/rmarkdown-libs/typedarray/typedarray.min.js"></script>
<script src="/rmarkdown-libs/jquery/jquery.min.js"></script>
<link href="/rmarkdown-libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="/rmarkdown-libs/crosstalk/js/crosstalk.min.js"></script>
<link href="/rmarkdown-libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="/rmarkdown-libs/plotly-main/plotly-latest.min.js"></script>


<p>Production and manufacturing processes are often very susceptible to quality fluctuations, as they depend on many parameters. For example, even small fluctuations of certain parameters can result in the fact that, for example, the dimensional accuracy of a workpiece no longer conforms. It is therefore necessary to monitor certain parameters. Common methods for this are mostly based on statistical methods (e.g. SPC or Six Sigma). However, what if a large part of the defects could already be detected during the production process, when all that is required for this is certain parameters that are continuously recorded by a production system.</p>
<p>This is exactly where I would like to start in my blog post today. I want to use the Random Forest classification method to implement a process control that detects whether a tool is worn out during machining or not.</p>
<blockquote>
<p>Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund &amp; R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance.<a href="https://link.springer.com/article/10.1023/A:1010933404324">Breiman, L. Random Forests. Machine Learning 45, 5–32 (2001)</a></p>
</blockquote>
<p>For this purpose, I will use a data set from the <a href="https://www.kaggle.com/shasun/tool-wear-detection-in-cnc-mill">University of Michigan</a>, which has recorded the wear of a tool in different machining environments and test runs. The records include over 25000 data sets described by 40 parameters.</p>
<pre class="r"><code>library(fs) # Loading the data
library(tidyverse) # General data wraggling
library(magrittr) # Piping
library(ranger) # Random Forest</code></pre>
<p>The implementation is done with the <a href="https://cran.r-project.org/web/packages/ranger/index.html">ranger</a> package, which allows a quick calculation.</p>
<p>Let’s start by loading the data into RStudio and making some adjustments. As a result, we will get the mill_data dataframe, which contains both meta-data and the test results.</p>
<pre class="r"><code>dir &lt;- fs::dir_info(path = &quot;C:/Users/User/Documents/18653_25987_bundle_archive/&quot;)

meta_data &lt;- dir %&gt;% 
  dplyr::filter(stringr::str_detect(path, &quot;train&quot;)) %&gt;% 
  dplyr::select(path) %&gt;% 
  dplyr::mutate(data = purrr::map(path, ~ read.csv(.))) %&gt;% 
  tidyr::unnest(data) %&gt;% 
  dplyr::select(-path) %&gt;% 
  dplyr::select(No, tool_condition)

raw_data &lt;- dir %&gt;% 
  dplyr::filter(stringr::str_detect(path, &quot;experiment&quot;)) %&gt;% 
  dplyr::select(path) %&gt;% 
  dplyr::mutate(No = dplyr::row_number(),
                data = purrr::map(path, ~ read.csv(.))) %&gt;% 
  tidyr::unnest(data) %&gt;% 
  dplyr::select(-path) %&gt;% 
  dplyr::select_at(vars(!contains(&quot;Position&quot;)))

mill_data &lt;- raw_data %&gt;%
  dplyr::left_join(meta_data, by = &quot;No&quot;) %&gt;% 
  dplyr::select(-No)

mill_data %&gt;% dplyr::glimpse()</code></pre>
<pre><code>## Rows: 25,286
## Columns: 41
## $ X1_ActualVelocity         &lt;dbl&gt; 0.0, -10.8, -17.8, -18.0, -17.9, -17.6, -...
## $ X1_ActualAcceleration     &lt;dbl&gt; 0.00, -350.00, -6.25, 0.00, -18.80, 81.20...
## $ X1_CommandVelocity        &lt;dbl&gt; 0.0, -13.6, -17.9, -17.9, -17.9, -17.9, -...
## $ X1_CommandAcceleration    &lt;dbl&gt; 0.00e+00, -3.58e+02, -9.54e-05, -9.54e-05...
## $ X1_CurrentFeedback        &lt;dbl&gt; 0.18, -10.90, -8.59, -6.11, -5.70, -5.85,...
## $ X1_DCBusVoltage           &lt;dbl&gt; 0.0207, 0.1860, 0.1400, 0.1300, 0.1140, 0...
## $ X1_OutputCurrent          &lt;dbl&gt; 329, 328, 328, 327, 328, 328, 328, 327, 3...
## $ X1_OutputVoltage          &lt;dbl&gt; 2.77, 23.30, 30.60, 30.30, 30.50, 30.90, ...
## $ X1_OutputPower            &lt;dbl&gt; -1.42e-06, 4.48e-03, 5.33e-03, 4.89e-03, ...
## $ Y1_ActualVelocity         &lt;dbl&gt; -0.025, -19.800, -32.500, -32.600, -32.00...
## $ Y1_ActualAcceleration     &lt;dbl&gt; -6.25, -750.00, 0.00, -62.50, 138.00, -12...
## $ Y1_CommandVelocity        &lt;dbl&gt; 0.0, -24.6, -32.3, -32.3, -32.3, -32.3, -...
## $ Y1_CommandAcceleration    &lt;dbl&gt; 0.00e+00, -6.47e+02, -9.54e-05, -9.54e-05...
## $ Y1_CurrentFeedback        &lt;dbl&gt; 0.539, -14.500, -7.790, -8.130, -13.800, ...
## $ Y1_DCBusVoltage           &lt;dbl&gt; 0.0167, 0.2810, 0.1390, 0.1560, 0.2020, 0...
## $ Y1_OutputCurrent          &lt;dbl&gt; 328, 325, 327, 325, 326, 326, 326, 325, 3...
## $ Y1_OutputVoltage          &lt;dbl&gt; 1.84, 37.80, 49.40, 47.60, 47.10, 49.60, ...
## $ Y1_OutputPower            &lt;dbl&gt; 6.43e-07, 1.26e-02, 9.43e-03, 1.05e-02, 1...
## $ Z1_ActualVelocity         &lt;dbl&gt; 0.0, -20.3, -33.7, -33.7, -33.6, -33.7, -...
## $ Z1_ActualAcceleration     &lt;dbl&gt; 0.00, -712.00, 37.50, -6.25, 18.80, -6.25...
## $ Z1_CommandVelocity        &lt;dbl&gt; 0.0, -25.6, -33.7, -33.7, -33.7, -33.7, -...
## $ Z1_CommandAcceleration    &lt;dbl&gt; 0.00e+00, -6.74e+02, -9.54e-05, 0.00e+00,...
## $ Z1_CurrentFeedback        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ Z1_DCBusVoltage           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ Z1_OutputCurrent          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ Z1_OutputVoltage          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ S1_ActualVelocity         &lt;dbl&gt; 0.00100, 0.00000, 0.00000, -0.00100, 0.00...
## $ S1_ActualAcceleration     &lt;dbl&gt; 0.2500, 0.2500, -0.4380, -0.2500, -0.1880...
## $ S1_CommandVelocity        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ S1_CommandAcceleration    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ S1_CurrentFeedback        &lt;dbl&gt; 0.52400, -0.28800, 0.52400, -0.18000, 0.2...
## $ S1_DCBusVoltage           &lt;dbl&gt; 2.74e-19, 2.74e-19, 2.74e-19, 2.74e-19, 2...
## $ S1_OutputCurrent          &lt;dbl&gt; 329, 328, 328, 328, 328, 328, 328, 328, 3...
## $ S1_OutputVoltage          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ S1_OutputPower            &lt;dbl&gt; 6.96e-07, -5.27e-07, 9.10e-07, 1.07e-06, ...
## $ S1_SystemInertia          &lt;dbl&gt; 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 1...
## $ M1_CURRENT_PROGRAM_NUMBER &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...
## $ M1_sequence_number        &lt;dbl&gt; 0, 4, 7, 7, 7, 7, 2, 7, 7, 7, 7, 7, 7, 7,...
## $ M1_CURRENT_FEEDRATE       &lt;dbl&gt; 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 5...
## $ Machining_Process         &lt;fct&gt; Starting, Prep, Prep, Prep, Prep, Prep, P...
## $ tool_condition            &lt;fct&gt; unworn, unworn, unworn, unworn, unworn, u...</code></pre>
<p>In the next step we generate a training data set and a test data set. The training data set should comprise 70% of the total data.</p>
<pre class="r"><code>set.seed(123)
mill_split &lt;- rsample::initial_split(mill_data, prop = .7)
mill_train &lt;- rsample::training(mill_split)
mill_test  &lt;- rsample::testing(mill_split)</code></pre>
<p>As features we select all parameters (columns) that do not reflect any positional information and the value we want to determine by our model (tool_condition).</p>
<pre class="r"><code>features &lt;- setdiff(names(mill_train), 
                    c(&quot;tool_condition&quot;, 
                      stringr::str_detect(names(mill_train), &quot;Position&quot;)))</code></pre>
<p>Now we create our Random Forest Model with:</p>
<ul>
<li><p>The formula: <code>tool_condition ~ .</code> which derives the state of the tool from the other parameters</p></li>
<li><p>Our training data record: <code>mill_train</code></p></li>
<li><p>The number of trees to be created: <code>num.tree</code></p></li>
<li><p>The number of variables to possibly split at in each node: <code>mtry</code></p></li>
<li><p>As well as the mode: <code>impurity</code> which allows us to make an evaluation of the weighting of the parameters (Gini index)</p></li>
</ul>
<pre class="r"><code>mill_ranger &lt;- ranger::ranger(
  formula   = tool_condition ~ ., 
  data      = mill_train, 
  num.trees = 500,
  mtry      = floor(length(features) / 3),
  importance      = &#39;impurity&#39;
)</code></pre>
<p><code>ranger</code> offers a number of additional parameters with which the model can be further optimized. In our case, we do not want to go any further into parameterization. It should be mentioned, however, that the <a href="https://cran.r-project.org/web/packages/tuneRanger/index.html">tuneRanger</a> package can be used to further optimize the model.</p>
<pre class="r"><code>mill_ranger$prediction.error</code></pre>
<pre><code>## [1] 0.127168</code></pre>
<p>Let us look at the prediction error of our model. This is 12.7%. So 87.3% of the data could be classified correctly, which is a good result without further optimization.</p>
<p>We know nothing more precise about the statement of the parameters. However, what we can determine using our random forest model is the effect of the parameters on the classification. According to our results, the parameters <code>S1_DCBusVoltage</code>, <code>X1_OutputCurrent</code>, <code>Y1_OutputCurrent</code> have the greatest influence.</p>
<pre class="r"><code>mill_ranger$variable.importance %&gt;% 
  dplyr::bind_rows() %&gt;% 
  tidyr::pivot_longer(cols = dplyr::everything()) %&gt;% 
  dplyr::arrange(desc(value)) %&gt;%
  dplyr::top_n(25) %&gt;%
  plotly::plot_ly(x = ~value,
                  y = ~reorder(name, value)) %&gt;% 
  plotly::add_bars() %&gt;% 
  plotly::layout(xaxis = list(title = &quot;Count&quot;),
                 yaxis = list(title = &quot;Top Parameters&quot;))</code></pre>
<pre><code>## Warning: `arrange_()` is deprecated as of dplyr 0.7.0.
## Please use `arrange()` instead.
## See vignette(&#39;programming&#39;) for more help
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_warnings()` to see where this warning was generated.</code></pre>
<div id="htmlwidget-1" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"visdat":{"1bf457041cc2":["function () ","plotlyVisDat"]},"cur_data":"1bf457041cc2","attrs":{"1bf457041cc2":{"x":{},"y":{},"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"bar","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"xaxis":{"domain":[0,1],"automargin":true,"title":"Count"},"yaxis":{"domain":[0,1],"automargin":true,"title":"Top Parameters","type":"category","categoryorder":"array","categoryarray":["Y1_CommandAcceleration","S1_OutputVoltage","Z1_ActualAcceleration","Y1_ActualVelocity","X1_ActualVelocity","Y1_ActualAcceleration","Y1_OutputPower","S1_OutputPower","X1_CurrentFeedback","S1_ActualAcceleration","Y1_CurrentFeedback","Y1_OutputVoltage","X1_ActualAcceleration","Machining_Process","X1_OutputPower","S1_CurrentFeedback","X1_OutputVoltage","X1_DCBusVoltage","S1_OutputCurrent","M1_CURRENT_FEEDRATE","M1_sequence_number","Y1_DCBusVoltage","Y1_OutputCurrent","X1_OutputCurrent","S1_DCBusVoltage"]},"hovermode":"closest","showlegend":false},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[732.35221990947,670.555190063958,587.508734353027,545.589663675432,498.092138991059,401.702594897214,399.080710799698,325.402444833124,304.984644477905,292.976792400881,282.782856223003,269.445666620046,266.283339230976,265.751510784839,253.400266235192,242.970184952054,239.864155661647,232.354548343786,219.227983703288,219.111042325736,210.230359408777,205.564868922393,187.422290075556,166.408458865974,113.653164266848],"y":["S1_DCBusVoltage","X1_OutputCurrent","Y1_OutputCurrent","Y1_DCBusVoltage","M1_sequence_number","M1_CURRENT_FEEDRATE","S1_OutputCurrent","X1_DCBusVoltage","X1_OutputVoltage","S1_CurrentFeedback","X1_OutputPower","Machining_Process","X1_ActualAcceleration","Y1_OutputVoltage","Y1_CurrentFeedback","S1_ActualAcceleration","X1_CurrentFeedback","S1_OutputPower","Y1_OutputPower","Y1_ActualAcceleration","X1_ActualVelocity","Y1_ActualVelocity","Z1_ActualAcceleration","S1_OutputVoltage","Y1_CommandAcceleration"],"type":"bar","orientation":"h","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"xaxis":"x","yaxis":"y","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>With the model we have created, we can now further classify during operation and determine whether a tool breakage has occurred. This can be done with the function predict, into which we now classify our test data (<code>mill_test</code>).</p>
<pre class="r"><code>predictions &lt;- predict(mill_ranger, mill_test, type = &quot;response&quot;)
head(predictions$predictions)</code></pre>
<pre><code>## [1] unworn unworn unworn unworn unworn unworn
## Levels: unworn worn</code></pre>
<p>Since I like to work with pipes, I would like to mention that the <a href="https://parsnip.tidymodels.org/articles/parsnip_Intro.html">parsnip</a> package (which is part of <a href="https://www.tidymodels.org/">tidymodell</a>) also contains a ranger implementation which allows an easy and understandable implementation.</p>
<pre class="r"><code>library(parsnip)

rf_with_seed &lt;-
  rand_forest(trees = 2000, 
              mtry = varying(), 
              mode = &quot;regression&quot;) %&gt;%
  set_engine(&quot;ranger&quot;, seed = 123) 

rf_with_seed %&gt;%
  set_args(mtry = 4) %&gt;%
  set_engine(&quot;ranger&quot;) %&gt;%
  fit(tool_condition ~ ., data = mill_train)</code></pre>
<p>Already with little effort we were able to create a classifier that detects worn tools to a large extent.</p>
<p>Fewer machine crashes, lower quality costs or an increase in OEE are only a few of the advantages that can be achieved with a classifier.</p>
