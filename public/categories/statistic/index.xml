<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistic | Handkecodet</title>
    <link>/categories/statistic/</link>
      <atom:link href="/categories/statistic/index.xml" rel="self" type="application/rss+xml" />
    <description>Statistic</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 13 Jun 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/me</url>
      <title>Statistic</title>
      <link>/categories/statistic/</link>
    </image>
    
    <item>
      <title>Detecting anomalies - a simple approach</title>
      <link>/post/detecting-anomalies-a-simple-approach/</link>
      <pubDate>Sat, 13 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/post/detecting-anomalies-a-simple-approach/</guid>
      <description>


&lt;p&gt;&lt;img src=&#34;C:/Users/User/Documents/handdke_codet/static/post/2020-06-13-outlier-detection_files/sheep-black-sheep.jpg&#34; width=&#34;775&#34; /&gt;&lt;/p&gt;
&lt;p&gt;During my work I have a lot to do with the analysis of time series. These are mainly signal data which have to be evaluated accordingly. Predictions have to be made, which give a conclusion on how a certain sensor value will behave in e.g. 30 days. This is only one of many applications of forecasting methods.&lt;/p&gt;
&lt;p&gt;Today, however, I would like to devote myself to another, equally exciting topic. As an analyst, I am not always interested in the future, but also in the past. For example, I’m interested in certain progressions, constellations, correlations or even outliers of data points.&lt;/p&gt;
&lt;p&gt;Outliers can occur in many use cases: Some are real and some are not. Maybe it is a sensor error or an error in the posting of stock items. Or it is actually an outlier that is a real deviation from the target. Such application cases are manifold:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A produced part is not within the specification&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The memory consumption of data centers deviates from normal&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Or fraud control by comparing the behaviour of a user&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A very useful definition of outliers can be found in the &lt;a href=&#34;https://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm&#34;&gt;Engineering statistics handbook&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;An outlier is an observation that lies an abnormal distance from other values in a random sample from a population. In a sense, this definition leaves it up to the analyst (or a consensus process) to decide what will be considered abnormal. Before abnormal observations can be singled out, it is necessary to characterize normal observations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There are many ways to identify outliers. The simplest is probably the graphical evaluation of progressions using a box plot or a time series. However, when it comes to checking data in large quantities and time-critically, such methods reach their limits. You need procedures that can be automated.&lt;/p&gt;
&lt;p&gt;In my contribution today I want to develop and test such a procedure.&lt;/p&gt;
&lt;p&gt;How can we now implement such an outlier detection. A common method is the determination by the inter-quantile range. By definition, inter-quantile range is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In descriptive statistics, the interquartile range (IQR), also called the midspread, middle 50%, or H‑spread, is a measure of statistical dispersion, being equal to the difference between 75th and 25th percentiles, or between upper and lower quartiles, IQR = Q3 − Q1.
&lt;a href=&#34;https://en.wikipedia.org/wiki/Interquartile_range&#34;&gt;wikipedia; 2020-06-14&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In order to determine these, we form the corresponding quantiles and then use them to calculate the IQR&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- runif(1000, 5.0, 7.5)

quantile &amp;lt;- stats::quantile(x, prob = c(0.25, 0.75), na.rm = TRUE)

quantile[[2]] - quantile[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.203253&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R also offers us with the stats package a function to calculate the value:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iqr &amp;lt;- stats::IQR(x)
iqr&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.203253&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to define the corresponding limits for outliers, a definition must be made of the value from which a data point is declared as an outlier. We use the following definition which is also used in the tsoutliers package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;limits &amp;lt;- quantile + 1.5 * iqr * c(-1, 1)
limits&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      25%      75% 
## 3.835619 8.648631&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our function should evaluate the column col of a dataframe and then output a new column outlier. This column is then indexed with TRUE or FALSE whether the corresponding value is an outlier.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;detect_outliers &amp;lt;- function(df, col) {
  
  col_enquo &amp;lt;- rlang::enquo(col)
  x &amp;lt;- df %&amp;gt;% dplyr::pull(!! col_enquo)
  
  quantile &amp;lt;- stats::quantile(x, prob = c(0.25, 0.75), na.rm = TRUE)
  iqr &amp;lt;- stats::IQR(x)
  limits &amp;lt;- quantile + 1.5 * iqr * c(-1, 1)

  tibble::tibble(Value = x) %&amp;gt;% 
    dplyr::mutate(lower_limit = limits[[1]],
                  upper_limit = limits[[2]],
                  outlier = dplyr::case_when(Value &amp;gt; upper_limit | 
                                               Value &amp;lt; lower_limit ~ TRUE,
                                             TRUE ~ FALSE)) %&amp;gt;% 
    dplyr::bind_cols(df %&amp;gt;% dplyr::select(- !! col_enquo))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s test our function on some sample data. Two outliers are built into the data set with 1002 points. One has the value -5, the other 20.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataframe &amp;lt;- tibble::tibble(Index = 1:1002,
                            Value = c(-5, runif(1000, 5.0, 7.5), 20))

outlier_df &amp;lt;- detect_outliers(dataframe, &amp;quot;Value&amp;quot;)
outlier_df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,002 x 5
##    Value lower_limit upper_limit outlier Index
##    &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;lgl&amp;gt;   &amp;lt;int&amp;gt;
##  1 -5           3.87        8.79 TRUE        1
##  2  5.96        3.87        8.79 FALSE       2
##  3  5.25        3.87        8.79 FALSE       3
##  4  6.45        3.87        8.79 FALSE       4
##  5  6.82        3.87        8.79 FALSE       5
##  6  6.19        3.87        8.79 FALSE       6
##  7  6.87        3.87        8.79 FALSE       7
##  8  5.86        3.87        8.79 FALSE       8
##  9  6.41        3.87        8.79 FALSE       9
## 10  6.56        3.87        8.79 FALSE      10
## # ... with 992 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us look at the result graphically again. In green we see the data points defined as outliers. The two horizontal lines mark the corresponding limits for the evaluation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;outlier_df %&amp;gt;% 
  ggplot2::ggplot(aes(x = Index)) + 
  ggplot2::geom_point(aes(y = Value, color = outlier)) + 
  ggplot2::geom_line(aes(y = lower_limit)) + 
  ggplot2::geom_line(aes(y = upper_limit)) +
  ggplot2::theme_light()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2020-06-13-detecting-anomalies-a-simple-approach_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see the two outliers are correctly classified by our outlier detection.&lt;/p&gt;
&lt;p&gt;Our chosen data behaves stationary. What if we want to examine data with a trend for outliers? Does our method still work?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataframe_trend &amp;lt;- tibble::tibble(Index = 1:1002,
                            Value = c(30, diffinv(rnorm(999, mean = 0, sd = 5)), 0))
outlier_df_trend &amp;lt;- detect_outliers(dataframe_trend, &amp;quot;Value&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;outlier_df_trend %&amp;gt;% 
  ggplot2::ggplot(aes(x = Index)) + 
  ggplot2::geom_point(aes(y = Value, color = outlier)) + 
  ggplot2::geom_line(aes(y = lower_limit)) + 
  ggplot2::geom_line(aes(y = upper_limit)) +
  ggplot2::theme_light()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;2020-06-13-detecting-anomalies-a-simple-approach_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see, our method fails with more complex processes. To solve the problem we need a different approach which I will describe in another post.&lt;/p&gt;
&lt;p&gt;My goal in this post was to show that outlier detection can be implemented relatively easily. It is important to note that the underlying data plays a decisive role.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lying with statistics</title>
      <link>/post/lying-with-statistics/</link>
      <pubDate>Mon, 20 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/post/lying-with-statistics/</guid>
      <description>


&lt;p&gt;&lt;img src=&#34;/post/2020-01-22-lying-with-statistics_files/games-dice-4715127_1920.jpg&#34; width=&#34;800&#34; /&gt;&lt;/p&gt;
&lt;p&gt;According to a &lt;a href=&#34;https://www.oxfordmartin.ox.ac.uk/downloads/academic/The_Future_of_Employment.pdf&#34;&gt;study&lt;/a&gt; from 2013, almost half of all jobs will be replaced by artificial intelligence by 2030. Sounds dramatic, and it would be, if the basis of this statement had not been completely out of the blue.&lt;/p&gt;
&lt;p&gt;The number of job losses (47%) comes from a study by economist Carl enedikt Frey and computer scientist Michael Osborne of Oxford University.&lt;/p&gt;
&lt;p&gt;In their study, they asked 10 experts (robotics and computer experts) how a selection of 70 professions could be automated and then extrapolated their results to 700 more professions.&lt;/p&gt;
&lt;p&gt;This finding is now known under the synonym “The Technology Trap”, but was initially published and disseminated by well-known newspapers (FAZ, Süddeutsche Zeitung, Zeit, etc.) without questioning the basics of the study.&lt;/p&gt;
&lt;p&gt;In the following I will go into detail about exactly these traps. The basis for this is the book by &lt;a href=&#34;https://www.goodreads.com/book/show/51291.How_to_Lie_with_Statistics&#34;&gt;Darrell Huff, How to lie with statistics&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;build-in-bias&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Build-in bias&lt;/h1&gt;
&lt;p&gt;The build in bias describes exactly the mistake Osborne and Frey made in their study. The foundation of the study is on shaky ground.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;How big is the population of the study?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What does the population represent?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Is the population representative?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the 1936 U.S. presidential election, a pre-election telephone poll was conducted to evaluate the chances of the candidates Landon and Roosevelt. Landon was ahead with 370 to 161 votes. The problem here: In those days, mostly rich people had their own phones. Other groups were not included. As we know today, the election was won by Roosevelt.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A report based on sampling must use a representative sample, which is one from which every source of bias has been removed.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;well-chosen-average&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Well chosen average&lt;/h1&gt;
&lt;p&gt;In the field of statistics, the concept of average is very vague. Behind it can be the arithmetic mean, the median or the mode. You can play with these terms and use them to influence the statement of a study in such a way that certain characteristics are more prominent or others are weakened.&lt;/p&gt;
&lt;p&gt;For normally distributed data data, the arithmetic mean, media and mode are the same. As soon as this is no longer the case, you provide deeper insights into data.&lt;/p&gt;
&lt;p&gt;Suppose a company announces that the average monthly salary of its employees is $5700. If the company has a large gap in the salary structure, it is likely to use the arithmetic mean for the specification. This means that there are few top earners who pull up the average. If you look at the median or the mode, however, you will notice that most of the people in the company are the ones who earn the least, namely $2000 and the median is $3000. Median is the middle number in a sorted list of numbers&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/2020-01-22-lying-with-statistics_files/huff1.jpg&#34; alt=&#34;Darrell Huff - How to lie with statistics&#34; width=&#34;800&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Darrell Huff - How to lie with statistics&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;standard-and-probable-error&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Standard and probable error&lt;/h1&gt;
&lt;p&gt;In various media one reads again and again something about the intelligence of important personalities, researchers or politicians. Irrespective of what the test’s statement is and which components it does not include, the individual values marked IQ are simply not meaningful. Assume that the intelligence of two people is compared: Person A has an IQ of 98 and Person B has an IQ of 101. Apparently, Person B is smarter than Person A. Like any other product of the sampling method IQ is a figure with a statistical error. Assuming the probable error is 3 points. Then person A would have an IQ of 95 to 101 with a 50% chance and person B would have an IQ of 98 to 104. So we see that such values cannot be considered as single, but more as a range. It could well be that person A is more tubby than person B.&lt;/p&gt;
&lt;p&gt;In statistics, more often than not, probable error is the standard error. With a simple standard deviation, ~68.2% of the values must lie within the normally distributed values. If you want to take a closer look, you can do so via wikipedia &lt;a href=&#34;https://en.wikipedia.org/wiki/Standard_error&#34;&gt;wikipedia&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I hope my short version could help to shed some light on the statistical traps that are sometimes buzzing around.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
